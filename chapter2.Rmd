# 2. Regression and model validation

*This chapter consist of exercises related data wrangling and analysis.*
However, the code for data wrangling is not presented here as instructed. After some house keeping, the exercise is divided to 5 parts. The contents of these parts are introduced in the beginning of each part.


I start with some house keeping and calling the packages used later in the exercise.
```{r House keeping}
rm(list=ls()) 
library(dplyr)
library(ggplot2)
library(GGally)
```
## Part 1: Importing data + brief overview
Here I start by importing the data that I produced in the data wrangling section.
Studying the data with functions dim() and str() reveal that the dataset includes 166 rows (observations) and 7 columns (variables).
Also, the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int). These things in the brackets describe the type of the variable.

```{r}
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int)
```

## Part 2: More detailed description about the data
This dataset is based on http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
For more detailed information about the data and variables in the original dataset used for data wrangling, please see http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
The data was originally collected as a questionnaire related to teaching and learning, and the questionnaire was done as an international research project made possible by Opettajien akatemiea (teachers' academy). 
After the data wrangling, our dataset used in the analysis ("students2014") includes two background variables ("age", "gender"), variables for exam points ("points") and attitude ("attitude") and three variables that describe indices derived from the original questionnaire and they are related to learning methods ("deep","stra", "surf").

Graphical overview and summary of the data:
```{r}
# initialize plot with data and aesthetic mapping
p1 <- ggplot(students2014, aes(x = attitude, y = points, col=gender))

# define the visualization type (points)
p2 <- p1 + geom_point()

# add a regression line
p3 <- p2 + geom_smooth(method = "lm")

# add a main title and draw the plot
p4 <- p3+ggtitle("Student's attitude versus exam points")
p4
```

The graph "Student's attitude versus exam points" suggests that there is a positive correlation with students attitude and exam points. This does not imply causality or statistical significance and the correlation is fairly difficult to perceive without drawing the lines.
```{r}
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20))) +ggtitle("Relationships between variables")

# draw the plot
p
```

A more detailed view is presented in the graph "Relationships between variables", which gives information about the distributions of variables and how they correlate with other variables. For example, from this graph we learn that exam points have the highest correlation with attitude and lowest with variable "deep" (in absolute values). The graph also shows that the sample includes much more women than men and more detailed differences between men and women in this sample can be found on the first row.
```{r}
#summary of variables
summary(students2014)

```
Studying the output of summary()-function we see the exact number of women (110) and men (56) in the sample and we learn about the distributions of variables. For example, average age in the sample is 25,51 while the youngest person in the sample was 17 and the oldest was 55.

## Part 3: linear model: choosing variables, interpretation and p-values 

In this part we study our linear model with exam points ("points") as a dependent variable. In the first model ("my_model"), the explanatory variables are "attitude", "stra" and "surf". These variables are chosen as they have highest correlation (in absolute values) with our dependent variable.

```{r}
# create a regression model with multiple explanatory variables
# chosen explanatory variables for the first model are "attitude", "stra" and "surf" as they have highest correlation with 
#our dependent variable "points" in absolute values
my_model <- lm(points ~ attitude + stra + surf, data = students2014)

# print out a summary of the model
summary(my_model)
```
Summary of our linear model suggests that in our model only the intercept and variable "attitude" have statistical significance (with P<0.01) in explaining "points". 

#### Interpretation of coefficients
The coefficients are interpreted such that an unit change in our explanatory variable is associated with a change size of the coefficient in the dependent variable ("points"). For example, one unit increase in "attitude" is associated with approximately 0.33 increase in exam points. Similarly, one unit increase in "stra"" is associated with 0.85 increase and one unit increase in "surf" is associated with 0.58 *decrease* in exam points.

#### Statistical tests related to the model parameters
The P-values indicate how likely (or unlikely) it is that our coefficients are different from zero. The null hypothesis (H0) is that the coefficients equal zero and the lower the p-value, the higher the chance that the real value of the coefficient differs from zero and the null hypothesis is rejected.

#### From model 1 to model 2: dropping one variable
As suggested in instructions, I remove variable "surf" from the model as it is not statistically significant. *The instructions were slightly ambivalent if I should remove one of non-significant variables or both. I chose to remove only one.* "surf" was chosen to be removed as it had lower correlation with "points" in absolute values and it is also further away from being statistically significant. The new model is called "my_model2". Interpretation follows in Part 4.

## Part 4: New linear model, interpretation of coefficients and multiple R-squared
In this part, we study the new model with two explanatory variables ("attitude" and "stra"), as suggested in the end of Part 3. 
```{r}
# new model without "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
```
#### Interpretation of coefficients and multiple R-squared
The summary of the new model suggests again that the intercept and variable "attitude" have statistical significance (now with P<0.001). Now, also "stra" is *somewhat* significant (0.05<P<0.10 is still often interpreted as not being statistically significant).
Now, one unit increase in "attitude" is associated with approximately 0.35 increase in exam points and one unit increase in "stra" is associated with approximately 0.91 increase in exam points.
Our model has multiple R-squared of 0.2048. The interpretation is that our model explains 20.48% of the variation of our dependent variable (exam points) around its mean. The higher the value, the more variation is explained by our model. 

The problem with R-squared is that it increases when more explanatory variables are added even if the new variables would make no sense. Thus, for multiple variable models Adjusted R-squared is useful as it punishes for adding explanatory variables and suggests adding more variables only if the new variable is really valuable such that the model is improved more than would be expected by chance.

## Part 5: Graphical model validation: diagnostics of the model
This part focuses on diagnostics of the model and the analysis is based on graphical model validation based on the following plots: Residuals vs Fitted values (first graph), Normal QQ-plot (second graph) and Residuals vs Leverage (third graph).
```{r}
plot(my_model2, which=c(1))
plot(my_model2, which=c(2))
plot(my_model2, which=c(5))
```

#### Assumptions of our model
Our model assumes that the relationship between our variables is linear as the dependent variable is modelled as a linear combination of model parameters. We also assume that the residuals

*  are normally distributed (with mean = 0 and variance=sigma^2 (which is constant)) 

*  are not correlated and that the size of a given error does not depend on the explanatory variables. 

Analysing the residuals allows us to study the validity of the assumptions.

### Studying the assumptions

### Constant variance of errors: Residuals vs Fitted values
The size of the errors should not depend on the explanatory variables. To inspect this, we look and the first graph (Residuals vs Fitted values), in which there seems to be a reasonably random spread and no significant patterns. This suggests that the size of the errors does not depend on the explanatory variables.

#### Normally distributed residuals: Normal QQ-plot
QQ-plot is used to study if the errors are normally distributed. In the second graph we see that our residuals fall pretty well to the line even though in the tails of the graph we see some deviations.The interpretation in our case is that the assumption of normally distributed errors is fairly reasonable.

#### Leverage of observations: Residuals vs Leverage
Our third graph shows graphically how much impact a single observation has on the model. As outliers in the data may have a strong impact on our model, we wish to study if our model is highly affected by few single observations. In our case, as suggested by the graph, there are no single outliers that would have huge impact on our model.

#### Conclusion about the assumptions
They seem to hold pretty well as suggested by the previous subsections.
