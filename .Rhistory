#Read data from the source mentioned above and name it as "lrn14"
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# look at the dimensions of the data
dim(lrn14)
#dataset includes 183 rows (observations) and 60 columns (variables)
# look at the structure of the data
str(lrn14)
#str() gives more detailed information about out dataset. We know that the set includes 183 obs. of  60 variables. Apart from variable "gender", all variables are integers.
#more detailed information about the variables can be found here: http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","Attitude", "deep", "stra", "surf", "Points")
str(keep_columns)
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014)
# print out the column names of the data
colnames(learning2014)
# change the name of the second column
colnames(learning2014)[2] <- "age"
# change the name of the third column
colnames(learning2014)[3] <- "attitude"
# change the name of "Points" to "points"
colnames(learning2014)[7]<-"points"
# print out the new column names of the data
colnames(learning2014)
# select male students
male_students <- filter(learning2014, gender == "M")
# select rows where points is greater than zero
learning2014 <- filter(learning2014, points >0)
#study the structure after data wrangling
str(learning2014)
#we now have 166 obs. of  7 variables as instructed in the instructions
?write.table
#After the wrangling we create and save the dataset to be used later for the analysis
write.table(learning2014, file="learning2014.txt", dec=".", sep=",")
setwd("~/Documents/GitHub/IODS-project")
#now lets see if previous step works as intended by importing the new file and studying its structure
learning2014<-read.table("learning2014.txt", dec=".", sep=",")
str(learning2014)
head(learning2014)
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
setwd("~/Documents/GitHub/IODS-project")
#Read data from the source mentioned above and name it as "lrn14"
students2014 <- read.table("learning2014.txt", sep="", header=TRUE)
#dimensions of the data
dim(students2014)
#dataset includes 183 rows (observations) and 60 columns (variables)
# look at the structure of the data
str(students2014)
#Ville Pikkarainen, November 2018
#This is an r-script for IODS-project. Source for the data: http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
#Read data from the source mentioned above and name it as "lrn14"
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# look at the dimensions of the data
dim(lrn14)
#dataset includes 183 rows (observations) and 60 columns (variables)
# look at the structure of the data
str(lrn14)
#str() gives more detailed information about out dataset. We know that the set includes 183 obs. of  60 variables. Apart from variable "gender", all variables are integers.
#more detailed information about the variables can be found here: http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","Attitude", "deep", "stra", "surf", "Points")
str(keep_columns)
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014)
# print out the column names of the data
colnames(learning2014)
# change the name of the second column
colnames(learning2014)[2] <- "age"
# change the name of the third column
colnames(learning2014)[3] <- "attitude"
# change the name of "Points" to "points"
colnames(learning2014)[7]<-"points"
# print out the new column names of the data
colnames(learning2014)
# select male students
male_students <- filter(learning2014, gender == "M")
# select rows where points is greater than zero
learning2014 <- filter(learning2014, points >0)
#study the structure after data wrangling
str(learning2014)
#we now have 166 obs. of  7 variables as instructed in the instructions
#After the wrangling we create and save the dataset to be used later for the analysis
write.table(learning2014, file="learning2014.txt", dec=".", sep=",")
setwd("~/Documents/GitHub/IODS-project")
#now lets see if previous step works as intended by importing the new file and studying its structure
learning2014<-read.table("learning2014.txt", dec=".", sep=",")
str(learning2014)
head(learning2014)
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
setwd("~/Documents/GitHub/IODS-project")
#Read data from the source mentioned above and name it as "lrn14"
students2014 <- read.table("learning2014.txt", , dec=".", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 183 rows (observations) and 60 columns (variables)
# look at the structure of the data
str(students2014)
library(ggplot2)
library(GGally)
install.packages("GGally")
#summary of variables
summary(students2014)
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
library(ggplot2)
library(GGally)
# PART 1
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", , dec=".", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int)
#############
# PART 2
#This dataset is based on http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
# !!! huom !!! "Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. "
#for more detailed information, please see http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
#graphical overview and summary of the data
# initialize plot with data and aesthetic mapping
p1 <- ggplot(students2014, aes(x = attitude, y = points, col=gender))
# define the visualization type (points)
p2 <- p1 + geom_point()
# draw the plot
p2
# add a regression line
p3 <- p2 + geom_smooth(method = "lm")
# add a main title and draw the plot
p4 <- p3+ggtitle("Student's attitude versus exam points")
p4
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
#summary of variables
summary(students2014)
# create an plot matrix with ggpairs()
ggpairs(students2014, lower = list(combo = wrap("facethist", bins = 20)))
############
# PART 3
#Show a summary of the fitted model and comment and interpret the results.
#Explain and interpret the statistical test related to the model parameters.
#If an explanatory variable in your model does not have a statistically significant
#relationship with the target variable, remove the variable from the model and
#fit the model again without it.
# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
# new model without "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + deep, data = students2014)
# print out a summary of the model
summary(my_model)
# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + age, data = students2014)
# print out a summary of the model
summary(my_model)
# create a regression model with multiple explanatory variables
# chosen explanatory variables for the first model are "attitude", "stra" and "surf" as they have highest correlation with
#our explanatory variable "points"
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
# new model without "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
# new model without "stra" and "surf"
my_model2 <- lm(points ~ attitude, data = students2014)
summary(my_model2)
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
#Read data from the source mentioned above and name it as "lrn14"
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# look at the dimensions of the data
dim(lrn14)
library(dplyr)
library(ggplot2)
library(GGally)
install.packages(c("lattice", "MASS", "Matrix", "ps", "survival", "tidyr", "xts"))
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
library(ggplot2)
library(GGally)
# PART 1
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", , dec=".", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int)
#############
# PART 2
#This dataset is based on http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
# !!! huom !!! "Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. "
#for more detailed information, please see http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
#graphical overview and summary of the data
# initialize plot with data and aesthetic mapping
p1 <- ggplot(students2014, aes(x = attitude, y = points, col=gender))
# define the visualization type (points)
p2 <- p1 + geom_point()
# draw the plot
p2
# add a regression line
p3 <- p2 + geom_smooth(method = "lm")
# add a main title and draw the plot
p4 <- p3+ggtitle("Student's attitude versus exam points")
p4
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
#summary of variables
summary(students2014)
# create an plot matrix with ggpairs()
ggpairs(students2014, lower = list(combo = wrap("facethist", bins = 20)))
############
# PART 3
#Show a summary of the fitted model and comment and interpret the results.
#Explain and interpret the statistical test related to the model parameters.
#If an explanatory variable in your model does not have a statistically significant
#relationship with the target variable, remove the variable from the model and
#fit the model again without it.
# create a regression model with multiple explanatory variables
# chosen explanatory variables for the first model are "attitude", "stra" and "surf" as they have highest correlation with
#our explanatory variable "points" in absolute values
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
#summary suggests that "stra" and "surf" dont have a statistically significant relationship with "points"
#Thus, I remove variables "surf" from the model.
# new model without  "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
#####
# PART 4
#Using a summary of your fitted model, explain the relationship between
#the chosen explanatory variables and the target variable (interpret the model parameters)
# Explain and interpret the multiple R squared of the model
####
# PART 5
#Produce the following diagnostic plots:
#Residuals vs Fitted values,
#Normal QQ-plot and
#Residuals vs Leverage.
plot(my_model2, which=c(1,2,5),par(mfrow = c(2,2)))
#assumptions based on the diagnostic plots.
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
library(ggplot2)
library(GGally)
# PART 1
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", dec=".", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
library(ggplot2)
library(GGally)
# PART 1
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
setwd("~/Documents/GitHub/IODS-project")
#Ville Pikkarainen, November 2018
#this is an r-script for data analysis for IODS-project's chapter 2
rm(list=ls())
# Access the dplyr library as it is needed later
library(dplyr)
library(ggplot2)
library(GGally)
# PART 1
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int)
#############
# PART 2
#This dataset is based on http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
# !!! huom !!! "Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. "
#for more detailed information, please see http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
#graphical overview and summary of the data
# initialize plot with data and aesthetic mapping
p1 <- ggplot(students2014, aes(x = attitude, y = points, col=gender))
# define the visualization type (points)
p2 <- p1 + geom_point()
# draw the plot
p2
# add a regression line
p3 <- p2 + geom_smooth(method = "lm")
# add a main title and draw the plot
p4 <- p3+ggtitle("Student's attitude versus exam points")
p4
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20))) +ggtitle("Relationships between variables")
# draw the plot
p
summary(students2014)
ggpairs(students2014, lower = list(combo = wrap("facethist", bins = 20)))
############
# create a regression model with multiple explanatory variables
# chosen explanatory variables for the first model are "attitude", "stra" and "surf" as they have highest correlation with
#our explanatory variable "points" in absolute values
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
# new model without  "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
plot(my_model2, which=c(1,2,5),par(mfrow = c(2,2)))
plot(my_model2, which=c(1,2,5),par(mfrow = c(3,1)))
plot(my_model2, which=c(5))
rm(list=ls())
library(dplyr)
library(ggplot2)
library(GGally)
```{r echo=FALSE}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(GGally)
rm(list=ls())
library(dplyr)
library(ggplot2)
library(GGally)
rm(list=ls())
library(dplyr)
library(ggplot2)
library(GGally)
# Regression and model validation
*This chapter consist of exercises related data wrangling and analysis.*
However, the code for data wrangling is not presented here as instructed.
## Data analysis
I start with some house keeping and calling the packages used later in the exercise.
```{r echo=FALSE}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(GGally)
```
### Part 1: Importing data + brief overview
Here I start by importing the data that I produced in the data wrangling section.
Studying the data with functions dim() and str() reveal that the dataset includes 166 rows (observations) and 7 columns (variables).
Also, the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int). These things in the brackets describe the type of the variable.
```{r}
#Read data from the source mentioned above and name it as "students14"
students2014 <- read.table("learning2014.txt", sep=",")
#dimensions of the data
dim(students2014)
#dataset includes 166 rows (observations) and 7 columns (variables)
# look at the structure of the data
str(students2014)
#the data frame includes variables "gender" (factor), "age" (int), "attitude"(int),"deep"(num),"stra"(num),"surf"(num) and "points"(int)
```
### Part 2: More detailed description about the data
This dataset is based on http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt
For more detailed information about the data and variables in the original dataset used for data wrangling, please see http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt
The data was originally collected as a questionnaire related to teaching and learning, and the questionnaire was done as an international research project made possible by Opettajien akatemiea (teachers' academy).
Graphical overview and summary of the data:
```{r}
# initialize plot with data and aesthetic mapping
p1 <- ggplot(students2014, aes(x = attitude, y = points, col=gender))
# define the visualization type (points)
p2 <- p1 + geom_point()
# draw the plot
p2
# add a regression line
p3 <- p2 + geom_smooth(method = "lm")
# add a main title and draw the plot
p4 <- p3+ggtitle("Student's attitude versus exam points")
p4
```
The graph "Student's attitude versus exam points" suggests that there is a positive correlation with students attitude and exam points. This does not imply causality or statistical significance and the correlation is fairly difficult to perceive without drawing the lines.
```{r}
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20))) +ggtitle("Relationships between variables")
# draw the plot
p
```
A more detailed view is presented in the graph "Relationships between variables", which gives information about the distributions of variables and how they correlate with other variables. For example, from this graph we learn that exam points have the highest correlation with attitude and lowest with variable "deep" (in absolute values). The graph also shows that the sample includes much more women than men and more detailed differences between men and women in this sample can be found on the first row.
```{r}
#summary of variables
summary(students2014)
```
Studying the output of summary()-function we see the exact number of women (110) and men (56) in the sample and we learn about the distributions of variables. For example, average age in the sample is 25,51 while the youngest person in the sample was 17 and the oldest was 55.
### Part 3: linear model: choosing variables, interpretation and p-values
In this part we study our linear model with exam points ("points") as a dependent variable. In the first model ("my_model"), the explanatory variables are "attitude", "stra" and "surf". These variables are chosen as they have highest correlation (in absolute values) with our dependent variable.
```{r}
# create a regression model with multiple explanatory variables
# chosen explanatory variables for the first model are "attitude", "stra" and "surf" as they have highest correlation with
#our dependent variable "points" in absolute values
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
# print out a summary of the model
summary(my_model)
```
Summary of our linear model suggests that in our model only the intercept and variable "attitude" have statistical significance (with P<0.01) in explaining "points".
##### Interpretation of coefficients
The coefficients are interpreted such that an unit change in our explanatory variable is associated with a change size of the coefficient in the dependent variable ("points"). For example, one unit increase in "attitude" is associated with approximately 0.33 increase in exam points. Similarly, one unit increase in "stra"" is associated with 0.85 increase and one unit increase in "surf" is associated with 0.58 *decrease* in exam points.
##### Statistical tests related to the model parameters
The P-values indicate how likely (or unlikely) it is that our coefficients are different from zero. The null hypothesis (H0) is that the coefficients equal zero and the lower the p-value, the higher the chance that the real value of the coefficient differs from zero and the null hypothesis is rejected.
##### From model 1 to model 2: dropping one variable
As suggested in instructions, I remove variable "surf" from the model as it is not statistically significant. *The instructions were slightly ambivalent if I should remove one of non-significant variables or both. I chose to remove only one.* "surf" was chosen to be removed as it had lower correlation with "points" in absolute values and it is also further away from being statistically significant. The new model is called "my_model2". Interpretation follows in Part 4.
### Part 4: New linear model, interpretation of coefficients and multiple R-squared
In this part, we study the new model with two explanatory variables ("attitude" and "stra"), as suggested in the end of Part 3.
```{r}
# new model without "surf"
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
```
##### Interpretation of coefficients and multiple R-squared
The summary of the new model suggests again that the intercept and variable "attitude" have statistical significance (now with P<0.001). Now, also "stra" is *somewhat* significant (0.05<P<0.10 is still often interpreted as not being statistically significant).
Now, one unit increase in "attitude" is associated with approximately 0.35 increase in exam points and one unit increase in "stra" is associated with approximately 0.91 increase in exam points.
Our model has multiple R-squared of 0.2048. The interpretation is that our model explains 20.48% of the variation of our dependent variable (exam points) around its mean. The higher the value, the more variation is explained by our model.
The problem with R-squared is that it increases when more explanatory variables are added even if the new variables would make no sense. Thus, for multiple variable models Adjusted R-squared is useful as it punishes for adding explanatory variables and suggests adding more variables only if the new variable is really valuable such that the model is improved more than would be expected by chance.
### Part 5: Graphical model validation: diagnostics of the model
This part focuses on diagnostics of the model and the analysis is based on graphical model validation based on the following plots: Residuals vs Fitted values (first graph), Normal QQ-plot (second graph) and Residuals vs Leverage (third graph).
```{r}
plot(my_model2, which=c(1,2,5),par(mfrow = c(3,1)))
##### Assumptions of our model
Our model assumes that the relationship between our variables is linear as the dependent variable is modelled as a linear combination of model parameters. We also assume that the residuals
-  are normally distributed (with \mu = 0 and \sigma^2 (which is constant))
-  are not correlated and that the size of a given error does not depend on the explanatory variables.
Analysing the residuals allows us to study the validity of the assumptions.
##### Studying the assumptions
##### Constant variance of errors: Residuals vs Fitted values
The size of the errors should not depend on the explanatory variables. To inspect this, we look and the first graph (Residuals vs Fitted values), in which there seems to be a reasonably random spread and no significant patterns. This suggests that the size of the errors does not depend on the explanatory variables.
###### Normally distributed residuals: Normal QQ-plot
QQ-plot is used to study if the errors are normally distributed. In the second graph we see that our residuals fall pretty well to the line even though in the tails of the graph we see some deviations.The interpretation in our case is that the assumption of normally distributed errors is fairly reasonable.
##### Leverage of observations: Residuals vs Leverage
Our third graph shows graphically how much impact a single observation has on the model. As outliers in the data may have a strong impact on our model, we wish to study if our model is highly affected by few single observations. In our case, as suggested by the graph, there are no single outliers that would have huge impact on our model.
##### Conclusion about the assumptions
They seem to hold pretty well as suggested by the previous subsections.
plot(my_model2, which=c(1)), main="Residuals vs Fitted values")
plot(my_model2, which=c(1)), main="Residuals vs Fitted values"))
plot(my_model2, which=c(1)) main="Residuals vs Fitted values"))
plot(my_model2, which=c(1)), main="Residuals vs Fitted values"))
plot(my_model2, which=c(1))
